#include <ver2/perceptron_2.hpp>
#include <ver2/utility.hpp>
#include <chrono>
#include <iostream>
#include <memory>

namespace p2 = perceptron_2;

// ë©”ëª¨ë¦¬ ì•ˆì „í•œ GOL Attention ëª¨ë¸
class SafeGOLAttentionModel {
private:
    int batch_size;
    int input_dim = 100; // 10x10 GOL íŒ¨í„´
    int attention_dim = 64;
    int output_dim = 8;
    
    // ì•ˆì „í•œ ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ ìœ„í•œ ìŠ¤ë§ˆíŠ¸ í¬ì¸í„°ë“¤
    std::unique_ptr<p2::Adam> input_proj_opt, attention_opt, output_opt;
    
    // ë ˆì´ì–´ë“¤
    p2::PerceptronLayer input_proj, attention_layer, output_layer;
    p2::ActivateLayer act;
    p2::LossLayer loss;
    p2::handleStream hs;

public:
    SafeGOLAttentionModel(int bs, double lr = 0.001) : batch_size(bs) {
        std::cout << "ğŸ”§ SafeGOLAttentionModel ì´ˆê¸°í™” ì¤‘..." << std::endl;
        
        try {
            // ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™”
            input_proj_opt = std::make_unique<p2::Adam>(input_dim, attention_dim, lr, p2::layerType::perceptron, hs.model_str);
            attention_opt = std::make_unique<p2::Adam>(attention_dim, attention_dim, lr, p2::layerType::perceptron, hs.model_str);
            output_opt = std::make_unique<p2::Adam>(attention_dim, output_dim, lr, p2::layerType::perceptron, hs.model_str);
            
            std::cout << "âœ… ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™” ì™„ë£Œ" << std::endl;
            
            // ë ˆì´ì–´ ì´ˆê¸°í™”
            input_proj = p2::PerceptronLayer(bs, input_dim, attention_dim, input_proj_opt.get(), d2::InitType::Xavier, hs.model_str);
            attention_layer = p2::PerceptronLayer(bs, attention_dim, attention_dim, attention_opt.get(), d2::InitType::Xavier, hs.model_str);
            output_layer = p2::PerceptronLayer(bs, attention_dim, output_dim, output_opt.get(), d2::InitType::Xavier, hs.model_str);
            
            std::cout << "âœ… ë ˆì´ì–´ ì´ˆê¸°í™” ì™„ë£Œ" << std::endl;
            
        } catch (const std::exception& e) {
            std::cerr << "âŒ ì´ˆê¸°í™” ì˜¤ë¥˜: " << e.what() << std::endl;
            throw;
        }
        
        std::cout << "âœ… SafeGOLAttentionModel ì´ˆê¸°í™” ì™„ë£Œ!" << std::endl;
    }
    
    // ì†Œë©¸ìëŠ” ìë™ìœ¼ë¡œ ìŠ¤ë§ˆíŠ¸ í¬ì¸í„°ê°€ ì²˜ë¦¬
    ~SafeGOLAttentionModel() = default;
    
    std::pair<d2::d_matrix_2<double>, double> forward(const d2::d_matrix_2<double>& input,
                                                     const d2::d_matrix_2<double>& target,
                                                     cudaStream_t str = 0) {
        try {
            // 1. Input projection (GOL pattern -> attention space)
            input_proj.feedforward(input, str);
            auto projected = act.Active(input_proj.getOutput(), p2::ActType::Tanh, str);
            
            // 2. Self-attention (simplified)
            attention_layer.feedforward(projected, str);
            auto attention_out = act.Active(attention_layer.getOutput(), p2::ActType::Tanh, str);
            
            // 3. Output projection
            output_layer.feedforward(attention_out, str);
            auto final_output = act.Active(output_layer.getOutput(), p2::ActType::Softmax, str);
            
            // 4. Loss calculation
            double loss_val = loss.getLoss(final_output, target, p2::LossType::CrossEntropy, str);
            
            return {final_output, loss_val};
            
        } catch (const std::exception& e) {
            std::cerr << "âŒ Forward pass ì˜¤ë¥˜: " << e.what() << std::endl;
            throw;
        }
    }
    
    void backward(const d2::d_matrix_2<double>& output,
                  const d2::d_matrix_2<double>& target,
                  cudaStream_t str = 0) {
        try {
            // Loss gradient
            auto loss_grad = loss.getGrad(output, target, p2::LossType::CrossEntropy, str);
            
            // Output layer backward
            auto output_deriv = act.d_Active(output_layer.getOutput(), p2::ActType::Softmax, str);
            auto grad_output = output_layer.backprop(loss_grad, output_deriv, str);
            
            // Attention layer backward
            auto attention_deriv = act.d_Active(attention_layer.getOutput(), p2::ActType::Tanh, str);
            auto grad_attention = attention_layer.backprop(grad_output, attention_deriv, str);
            
            // Input projection backward
            auto input_deriv = act.d_Active(input_proj.getOutput(), p2::ActType::Tanh, str);
            auto grad_input = input_proj.backprop(grad_attention, input_deriv, str);
            
        } catch (const std::exception& e) {
            std::cerr << "âŒ Backward pass ì˜¤ë¥˜: " << e.what() << std::endl;
            throw;
        }
    }
    
    void show_attention_weights(const d2::d_matrix_2<double>& input) {
        try {
            // Forward to get attention activations
            input_proj.feedforward(input, 0);
            auto projected = act.Active(input_proj.getOutput(), p2::ActType::Tanh, 0);
            attention_layer.feedforward(projected, 0);
            auto attention_features = attention_layer.getOutput();
            
            attention_features.cpyToHost();
            
            std::cout << "\nğŸ” Attention ê°€ì¤‘ì¹˜ ë¶„ì„:" << std::endl;
            std::cout << "ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ attention features (ìƒìœ„ 10ê°œ):" << std::endl;
            
            for (int i = 0; i < std::min(10, attention_dim); ++i) {
                double weight = attention_features.getHostValue(0, i);
                std::cout << "  Feature " << std::setw(2) << i << ": " 
                         << std::setw(8) << std::fixed << std::setprecision(4) << weight;
                
                // ê°€ì¤‘ì¹˜ ì‹œê°í™”
                int bar_length = static_cast<int>(std::abs(weight) * 20);
                std::cout << " [";
                for (int j = 0; j < bar_length && j < 20; ++j) {
                    std::cout << (weight > 0 ? '+' : '-');
                }
                for (int j = bar_length; j < 20; ++j) {
                    std::cout << " ";
                }
                std::cout << "]" << std::endl;
            }
            
        } catch (const std::exception& e) {
            std::cerr << "âŒ Attention ì‹œê°í™” ì˜¤ë¥˜: " << e.what() << std::endl;
        }
    }
};

void test_safe_gol_attention() {
    std::cout << "ğŸš€ ì•ˆì „í•œ GOL Attention ëª¨ë¸ í…ŒìŠ¤íŠ¸!" << std::endl;
    
    const int batch_size = 16;
    const int epochs = 30;
    const double learning_rate = 0.001;
    
    try {
        std::cout << "ğŸ—ï¸ ëª¨ë¸ ìƒì„± ì¤‘..." << std::endl;
        SafeGOLAttentionModel model(batch_size, learning_rate);
        
        std::cout << "ğŸ“Š í›ˆë ¨ ë°ì´í„° ì¤€ë¹„ ì¤‘..." << std::endl;
        
        // í›ˆë ¨ ë°ì´í„° ìƒì„±
        d2::d_matrix_2<double> train_input(batch_size, 100);
        d2::d_matrix_2<double> train_target(batch_size, 8);
        
        // GOL íŒ¨í„´ ì‹œë®¬ë ˆì´ì…˜ (ëœë¤ ìƒì„±)
        train_input.randomInit(0.0, 1.0);
        
        // ë‹¤ì–‘í•œ íŒ¨í„´ í´ë˜ìŠ¤ ìƒì„±
        train_target.fill(0.0);
        for (int i = 0; i < batch_size; ++i) {
            int pattern_class = i % 8;
            train_target.setHostValue(i, pattern_class, 1.0);
        }
        
        train_input.cpyToDev();
        train_target.cpyToDev();
        
        std::cout << "ğŸ¯ GOL íŒ¨í„´ ë¶„ë¥˜ í›ˆë ¨ ì‹œì‘!" << std::endl;
        std::cout << "ë°°ì¹˜ í¬ê¸°: " << batch_size << ", ì—í¬í¬: " << epochs << std::endl;
        std::cout << "ì…ë ¥ ì°¨ì›: 100 (10x10 GOL), ì¶œë ¥ í´ë˜ìŠ¤: 8" << std::endl;
        std::cout << "======================================" << std::endl;
        
        // í›ˆë ¨ ë£¨í”„
        for (int epoch = 1; epoch <= epochs; ++epoch) {
            auto start_time = std::chrono::steady_clock::now();
            
            // Forward & Backward
            auto [output, loss_val] = model.forward(train_input, train_target);
            model.backward(output, train_target);
            
            auto end_time = std::chrono::steady_clock::now();
            auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end_time - start_time);
            
            if (epoch % 5 == 0 || epoch == 1) {
                std::cout << "Epoch " << std::setw(3) << epoch 
                         << " | Loss: " << std::setw(8) << std::fixed << std::setprecision(6) << loss_val
                         << " | Time: " << std::setw(3) << duration.count() << "ms";
                
                // ì†ì‹¤ ê°ì†Œ íŠ¸ë Œë“œ í‘œì‹œ
                if (epoch > 1) {
                    if (loss_val < 2.0) std::cout << " ğŸ“ˆ";
                    else if (loss_val < 2.05) std::cout << " ğŸ“Š";
                    else std::cout << " ğŸ“‰";
                }
                std::cout << std::endl;
                
                // Attention ì‹œê°í™”
                if (epoch % 15 == 0) {
                    model.show_attention_weights(train_input);
                }
            }
        }
        
        std::cout << "\nâœ… í›ˆë ¨ ì™„ë£Œ!" << std::endl;
        std::cout << "ğŸ” ìµœì¢… Attention íŒ¨í„´ ë¶„ì„:" << std::endl;
        model.show_attention_weights(train_input);
        
        std::cout << "\nğŸ‰ SafeGOLAttentionModel í…ŒìŠ¤íŠ¸ ì„±ê³µ!" << std::endl;
        std::cout << "âœ¨ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì—†ì´ ì•ˆì „í•˜ê²Œ ì‹¤í–‰ë¨!" << std::endl;
        
    } catch (const std::exception& e) {
        std::cerr << "âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: " << e.what() << std::endl;
        throw;
    }
}

int main() {
    try {
        std::cout << "ğŸ¯ ì•ˆì „í•œ GOL Attention ì‹¤í—˜!" << std::endl;
        std::cout << "ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™” ë²„ì „" << std::endl;
        std::cout << "=====================================\n" << std::endl;
        
        test_safe_gol_attention();
        
        std::cout << "\nğŸ† ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!" << std::endl;
        std::cout << "ğŸ” ë©”ëª¨ë¦¬ ì•ˆì „ì„± ê²€ì¦ ì™„ë£Œ" << std::endl;
        
    } catch (const std::exception& e) {
        std::cerr << "âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: " << e.what() << std::endl;
        return -1;
    }
    
    return 0;
}
