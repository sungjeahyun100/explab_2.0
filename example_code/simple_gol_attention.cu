#include <ver2/perceptron_2.hpp>
#include <ver2/utility.hpp>
#include <chrono>
#include <iostream>
#include <memory>

namespace p2 = perceptron_2;

// ê°„ë‹¨í•œ GOL Attention í…ŒìŠ¤íŠ¸ (ë©”ëª¨ë¦¬ ë¬¸ì œ í•´ê²° ë²„ì „)
class SimpleGOLAttention {
private:
    int batch_size;
    int input_dim;
    int hidden_dim;
    
    std::unique_ptr<p2::Adam> fc1_opt, fc2_opt, attention_opt;
    p2::PerceptronLayer fc1, fc2, attention_layer;
    p2::ActivateLayer act;
    p2::LossLayer loss;
    p2::handleStream hs;

public:
    SimpleGOLAttention(int bs, int input_d = 100, int hidden_d = 64, double lr = 0.001) 
        : batch_size(bs), input_dim(input_d), hidden_dim(hidden_d) {
        
        // ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™”
        fc1_opt = std::make_unique<p2::Adam>(input_d, hidden_d, lr, p2::layerType::perceptron, hs.model_str);
        fc2_opt = std::make_unique<p2::Adam>(hidden_d, hidden_d, lr, p2::layerType::perceptron, hs.model_str);
        attention_opt = std::make_unique<p2::Adam>(hidden_d, 8, lr, p2::layerType::perceptron, hs.model_str);
        
        // ë ˆì´ì–´ ì´ˆê¸°í™”
        fc1 = p2::PerceptronLayer(bs, input_d, hidden_d, fc1_opt.get(), d2::InitType::Xavier, hs.model_str);
        fc2 = p2::PerceptronLayer(bs, hidden_d, hidden_d, fc2_opt.get(), d2::InitType::Xavier, hs.model_str);
        attention_layer = p2::PerceptronLayer(bs, hidden_d, 8, attention_opt.get(), d2::InitType::Xavier, hs.model_str);
        
        std::cout << "âœ… SimpleGOLAttention ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ!" << std::endl;
    }
    
    std::pair<d2::d_matrix_2<double>, double> forward(const d2::d_matrix_2<double>& input,
                                                     const d2::d_matrix_2<double>& target,
                                                     cudaStream_t str = 0) {
        
        // ì²« ë²ˆì§¸ FC ë ˆì´ì–´
        fc1.feedforward(input, str);
        auto fc1_out = act.Active(fc1.getOutput(), p2::ActType::ReLU, str);
        
        // ë‘ ë²ˆì§¸ FC ë ˆì´ì–´ (attention-like processing)
        fc2.feedforward(fc1_out, str);  
        auto fc2_out = act.Active(fc2.getOutput(), p2::ActType::Tanh, str);
        
        // Attention ì¶œë ¥ ë ˆì´ì–´
        attention_layer.feedforward(fc2_out, str);
        auto final_output = act.Active(attention_layer.getOutput(), p2::ActType::Softmax, str);
        
        // ì†ì‹¤ ê³„ì‚°
        double loss_val = loss.getLoss(final_output, target, p2::LossType::CrossEntropy, str);
        
        return {final_output, loss_val};
    }
    
    void backward(const d2::d_matrix_2<double>& output,
                  const d2::d_matrix_2<double>& target,
                  cudaStream_t str = 0) {
        
        // ì†ì‹¤ ê¸°ìš¸ê¸°
        auto loss_grad = loss.getGrad(output, target, p2::LossType::CrossEntropy, str);
        
        // Attention ë ˆì´ì–´ ì—­ì „íŒŒ
        auto attention_deriv = act.d_Active(attention_layer.getOutput(), p2::ActType::Softmax, str);
        auto grad_attention = attention_layer.backprop(loss_grad, attention_deriv, str);
        
        // FC2 ì—­ì „íŒŒ
        auto fc2_deriv = act.d_Active(fc2.getOutput(), p2::ActType::Tanh, str);
        auto grad_fc2 = fc2.backprop(grad_attention, fc2_deriv, str);
        
        // FC1 ì—­ì „íŒŒ
        auto fc1_deriv = act.d_Active(fc1.getOutput(), p2::ActType::ReLU, str);
        auto grad_fc1 = fc1.backprop(grad_fc2, fc1_deriv, str);
    }
    
    void show_attention_pattern(const d2::d_matrix_2<double>& input) {
        auto [output, _] = forward(input, d2::d_matrix_2<double>(batch_size, 8));
        
        // ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ attention ê°€ì¤‘ì¹˜ ì¶œë ¥
        fc2.feedforward(act.Active(fc1.getOutput(), p2::ActType::ReLU), 0);
        auto attention_features = fc2.getOutput();
        
        attention_features.cpyToHost();
        
        std::cout << "\nğŸ” Attention íŒ¨í„´ (ì²« ë²ˆì§¸ ë°°ì¹˜):" << std::endl;
        std::cout << "Hidden features (first 10 values): ";
        for (int i = 0; i < std::min(10, hidden_dim); ++i) {
            std::cout << std::fixed << std::setprecision(4) 
                     << attention_features.getHostValue(0, i) << " ";
        }
        std::cout << std::endl;
    }
};

void test_simple_gol_attention() {
    std::cout << "ğŸš€ ê°„ë‹¨í•œ GOL Attention í…ŒìŠ¤íŠ¸ ì‹œì‘!" << std::endl;
    
    const int batch_size = 16;
    const int input_dim = 100; // 10x10 GOL íŒ¨í„´
    const int epochs = 20;
    
    try {
        SimpleGOLAttention model(batch_size, input_dim, 64);
        
        // í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        d2::d_matrix_2<double> train_input(batch_size, input_dim);
        d2::d_matrix_2<double> train_target(batch_size, 8);
        
        std::cout << "ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì¤‘..." << std::endl;
        
        // ëœë¤ GOL íŒ¨í„´ ìƒì„±
        train_input.randomInit(0.0, 1.0);
        
        // íƒ€ê²Ÿ ì„¤ì • (ë¶„ë¥˜ ë¬¸ì œ)
        train_target.fill(0.0);
        for (int i = 0; i < batch_size; ++i) {
            int target_class = i % 8; // 8ê°œ í´ë˜ìŠ¤ ìˆœí™˜
            train_target.setHostValue(i, target_class, 1.0);
        }
        
        train_input.cpyToDev();
        train_target.cpyToDev();
        
        std::cout << "ğŸ”„ í›ˆë ¨ ì‹œì‘..." << std::endl;
        
        // í›ˆë ¨ ë£¨í”„
        for (int epoch = 1; epoch <= epochs; ++epoch) {
            auto start_time = std::chrono::steady_clock::now();
            
            // Forward pass
            auto [output, loss_val] = model.forward(train_input, train_target);
            
            // Backward pass
            model.backward(output, train_target);
            
            auto end_time = std::chrono::steady_clock::now();
            auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end_time - start_time);
            
            if (epoch % 5 == 0 || epoch == 1) {
                std::cout << "Epoch " << std::setw(2) << epoch 
                         << " | Loss: " << std::setw(8) << std::fixed << std::setprecision(6) << loss_val
                         << " | Time: " << std::setw(3) << duration.count() << "ms" << std::endl;
                
                if (epoch % 10 == 0) {
                    model.show_attention_pattern(train_input);
                }
            }
        }
        
        std::cout << "\nâœ… ê°„ë‹¨í•œ GOL Attention í…ŒìŠ¤íŠ¸ ì™„ë£Œ!" << std::endl;
        model.show_attention_pattern(train_input);
        
    } catch (const std::exception& e) {
        std::cerr << "âŒ ì˜¤ë¥˜ ë°œìƒ: " << e.what() << std::endl;
        throw;
    }
}

int main() {
    try {
        std::cout << "ğŸ¯ ê°„ë‹¨í•œ GOL Attention ì‹¤í—˜!" << std::endl;
        
        test_simple_gol_attention();
        
        std::cout << "\nğŸ‰ ì‹¤í—˜ ì™„ë£Œ!" << std::endl;
        std::cout << "âœ¨ ë©”ëª¨ë¦¬ ë¬¸ì œ ì—†ì´ ì •ìƒ ì‘ë™!" << std::endl;
        
    } catch (const std::exception& e) {
        std::cerr << "âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: " << e.what() << std::endl;
        return -1;
    }
    
    return 0;
}
