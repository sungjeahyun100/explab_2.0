이 문서는 현제 이 코드베이스가 어떤 목적으로 만들어졌는지 인공지능에게 설명하기 위해 작성된 문서이다.

이 코드베이스는 필자의 호기심에 의해 생긴 여러 논제를 파악하기 위한 실험을 하기 위해 만들어졌다.
아레엔 현제 진행중인 실험의 일련번호, 이름과 기원이 된 논제, 구체적인 실험 내용과 그리고 현제 당면한 과제가 작성되어 있다. 

id:exp-001
name:project mission impossible
based topic:풀지 못함이 수학적으로 증명된 문제를 인공지능이 풀이를 시도한다면 인공지능은 어떤 결론을 내릴 것인가? 

attampt 1: 분류형 신경망 모델(mlp)에게 초기조건만 쥐어준 후 미래를 예측하도록 훈련시켜본다.
우선 가장 잘 훈련되는 크기 및 설정을 찾은 후, 데이터를 점차적으로 늘려가면서 loss율의 추이를 관찰한다. 

attampt 1 detail:
GOL의 예측 불가능성을 활용하여 데이터셋을 생성하였다.
사용된 데이터 입력: 10*10 무작위 GOL패턴.
목표값:8비트 이진수 형태의 입력패턴의 살아남은 셀 개수(이때 이 개수는 100*100 보드에서 2500세데 또는 패턴 안정화가 될 때까지 세데를 진행시킨 후 살아있는 셀의 개수를 셈으로서 계산하였다.)

최적 설정 실험에서 쓰인 데이터의 양:1000개, epoch설정:100
추이 관찰 실험은 최적화의 한계로 시간이 오래걸려 진행이 불가함.

founded optimized model details:
simple MLP(Adam opt, not mini-batched) 
input layer 100 -> 128, LReLU act
hidden1 layer 128 -> 128, tanh act
hidden2 layer 128 -> 64, tanh act
output layer 64 -> 8, Softsign act
loss calc: cross entropy

reaches minimum loss val: 0.62

당면한 과제: 
목표값 설정의 타당성이 의심된다. 현제 살아있는 셀의 개수가 min:0, max:450이상인데, 분포 그래프를 보면 지나치게 50 미만의 경우로 쏠려있다.(explab_ver2/past_datagraph 경로에 파일을 올려뒀다.)
따라서 세데를 진행시킨 100*100 패턴 자체를 타겟으로 잡을 계획이다.
여기서 총 세 가지 문제가 생긴다.
(1) 아웃풋이 지나치게 커 모델이 컴퓨팅 파워를 너무 많이 잡아먹음.
(2) 안정화된 패턴이 애초에 너무 작아 제데로 된 훈련이 안될 수도 있다는 점.
(3) 데이터셋 생성 자체가 너무 무거워져 추이 관찰 실험이 불가능할 수도 있다는 점(현제 계획 상으론 1000 -> 2000 -> 4000 -> 8000 ... 이렇게 비선형적으로 파일 개수를 늘려나가는 것, epoch수도 1000으로 늘릴 생각이였다.)    


